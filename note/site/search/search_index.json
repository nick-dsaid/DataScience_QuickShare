{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Welcome to MkDocs"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/KNN/","text":"--- Theory and Intuition ![[Pasted image 20220217233400.png]] - Tie considerations and options - Always choose an Odd K - Simply reduce K by 1 until tie is broken - Randomly break tie - Choose the nearest class point What does Scikit-Learn do in case of tie? Warning: - Regarding the Nearest Neighbors algorithms, if it is found that two neighbors, neighbor k+1 and k, have identical distances but different labels, the results wil depend on the ==ordering of the training data.== - In the case of ties, the answer will be the class that happens to appear first in the set of neighbors. - Results are ordered by distance, so it chooses the class of the closest point. - ![[Pasted image 20220217233856.png]] We want a K value that mininmizes error Error = 1- Accuracy Two Methods Elbow method. Cross validate a grid search of multiple K values and choose K that results in lowest error or highest accuracy. Cross validation only takes into account the K value with the lowest error rate across multiple folds. This could result in a more complex model (higher value of K). Consider the context of the problem to decide if larger K values are an issue. ![[Pasted image 20220217234534.png]] KNN - Distance Metric to Measure Distance Minkowski Euclidean Manhattan Chebyshev Scaling is necessary for KNN --- Extra: Pipeline in Scikit-Learn scaler = StandardScaler() knn = KNeighborsClassifier() knn.get_params().keys() # The string and the object name must match operations = [('scaler', scaler), ('knn', knn)] pipe = Pipeline(operations) from sklearn.model_selection import GridSearchCV # if your parameter grid is going inside a Pipeline, your parameter name needs to be specified in the following manner: # - chosen_string_name + two_underscores + parameter_key_name # - model_name + __ + parameter_name param_grid = {'knn__n_neighbors': k_values, 'knn_metric': ['']} full_cv_classifier = GridSearchCV(pipe, param_grid, cv=5, scoring='accuracy') full_cv_classifier.fit(X_train, y_train) # Notice the scaler is already included full_cv_classifier.predict(X_test)","title":"KNN"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/KNN/#-","text":"","title":"---"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/KNN/#theory-and-intuition","text":"![[Pasted image 20220217233400.png]] - Tie considerations and options - Always choose an Odd K - Simply reduce K by 1 until tie is broken - Randomly break tie - Choose the nearest class point What does Scikit-Learn do in case of tie? Warning: - Regarding the Nearest Neighbors algorithms, if it is found that two neighbors, neighbor k+1 and k, have identical distances but different labels, the results wil depend on the ==ordering of the training data.== - In the case of ties, the answer will be the class that happens to appear first in the set of neighbors. - Results are ordered by distance, so it chooses the class of the closest point. - ![[Pasted image 20220217233856.png]] We want a K value that mininmizes error Error = 1- Accuracy Two Methods Elbow method. Cross validate a grid search of multiple K values and choose K that results in lowest error or highest accuracy. Cross validation only takes into account the K value with the lowest error rate across multiple folds. This could result in a more complex model (higher value of K). Consider the context of the problem to decide if larger K values are an issue. ![[Pasted image 20220217234534.png]] KNN - Distance Metric to Measure Distance Minkowski Euclidean Manhattan Chebyshev Scaling is necessary for KNN","title":"Theory and Intuition"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/KNN/#-_1","text":"","title":"---"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/KNN/#extra-pipeline-in-scikit-learn","text":"scaler = StandardScaler() knn = KNeighborsClassifier() knn.get_params().keys() # The string and the object name must match operations = [('scaler', scaler), ('knn', knn)] pipe = Pipeline(operations) from sklearn.model_selection import GridSearchCV # if your parameter grid is going inside a Pipeline, your parameter name needs to be specified in the following manner: # - chosen_string_name + two_underscores + parameter_key_name # - model_name + __ + parameter_name param_grid = {'knn__n_neighbors': k_values, 'knn_metric': ['']} full_cv_classifier = GridSearchCV(pipe, param_grid, cv=5, scoring='accuracy') full_cv_classifier.fit(X_train, y_train) # Notice the scaler is already included full_cv_classifier.predict(X_test)","title":"Extra: Pipeline in Scikit-Learn"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Linear%20Regression/","text":"[[#Overview|Overview]] [[#Understanding OLS|Understanding OLS]] [[#Formula for OLS (Intuitive)|Formula for OLS (Intuitive)]] [[#Formula for OLS (Equation)|Formula for OLS (Equation)]] [[#Limitations of Linear Regression|Limitations of Linear Regression]] [[#Understand Cost Function|Understand Cost Function]] [[#Mean Squared Error (MSE)|Mean Squared Error (MSE)]] [[#Gradient Descent|Gradient Descent]] [[#Intro to Linear Regression|Intro to Linear Regression]] [[#To Walkthrough an Single Feature Example|To Walkthrough an Single Feature Example]] [[#To Walkthrough an Two Feature Example|To Walkthrough an Two Feature Example]] [[#Linear Regression in Python|Linear Regression in Python]] [[#Simple Linear Regression|Simple Linear Regression]] [[#Overview on Scikit-Learn|Overview on Scikit-Learn]] [[#Evaluating Regression|Evaluating Regression]] [[#Mean Absolute Error|Mean Absolute Error]] [[#Mean Squared Error|Mean Squared Error]] [[#Root Mean Square Error|Root Mean Square Error]] Overview ![[Pasted image 20220208231922.png]] ![[Pasted image 20220212124652.png]] Understanding OLS using Simple Linear Regression as example OLS -> y=mx+b There is only room for one possible feature x At later part will see there is a need like gradient descent to scale this to multiple features ![[Pasted image 20220208233744.png]] ![[Pasted image 20211013004110.png]] ![[Pasted image 20220208233918.png]] ![[Pasted image 20220208233956.png]] title: y hat (Not y) Thereis usuallky no set of Betas to create a perfect fit to y! Formula for OLS (Intuitive) ![[msedge_EMtik5iNnt.png]] title: Intuitive Explanation for OLS Slope of the regression line is the weighted average of $$ \\frac{yi-\\bar{y}}{x^i - \\bar{x}}$$ Formula for OLS (Equation) ![[Pasted image 20220208235124.png]] Limitations of Linear Regression ![[msedge_avdWgYmrsy.png]] ![[Pasted image 20211013001800.png]] Understand Cost Function What we know so far: - Linear Relationships - y mx+b - OLS - Solve simple linear regression - Not scalable for multiple features - Translating real data to Matrix Notation - Generalized formula for Beta coefficients Mean Squared Error (MSE) title: Average Squared Error for **m** rows $$\\frac{1}{m}\\sum^m_{j=1}(y^j - \\hat{y}^j)^2$$ title: Cost Function Define by Squared Error $$ J(\\beta) = \\frac{1}{2m}\\sum^m_{j=1}(y^j - \\hat{y}^j)^2$$ Function below where the y_hat is replace with the generalized function $$ J(\\beta) = \\frac{1}{2m}\\sum^m_{j=1}(y^j - \\sum^n_{i=0}\\beta_ix^j_i)^2$$ - The 2m is because taking the derivative by setting to 0 (canccel with power of 2) title: Additional 1/2 is for convenience for derivative $$J(\\beta) = \\frac{1}{2m}\\sum^m_{j=1}(y^j - \\hat{y}^j)^2$$ Then to substitute y_hat $$J(\\beta) = \\frac{1}{2m}\\sum^m_{j=1}(y^j - \\sum^n_{i=0}\\beta_ix^j_i)^2$$ ![[Pasted image 20220212113805.png]] - Unforunately, it's not sacalable to try to get an analytical solution to minize this cost function - Later part will learn to use gradient descent to minimize the cost functiion here Gradient Descent Intro to Linear Regression Now we have the Cost Function to minimzize Taking the cost function derivative and then solving for zero to get the set of Beta Coefficients wioll be too difficult to solve directly through analytical solution Instead we describe this Cost Function through vectorized matrix notation and use gradient descent to have a computer figure out the set of Beta Coefficient values that mininize the cost/loss function . title: Recall the Cost Function (Rewritten) - where $k$ is the number of features $$ \\frac{\\partial J}{\\partial a\\beta_k}(\\beta) = \\frac{1}{m} \\sum^m_{j=1} (y^j - \\sum^n_{i=0}\\beta_i x^j_i) (-x^j_k)$$ - Also recall the data is in the form of a **matrix X** with a **vector of labels y** - Whcih means if we need a $\\beta$ for each feature, we can express a vector $\\beta$ value - That means we can plugin our equation of the derivative of the loss function - ![[Pasted image 20220212115241.png]] - ![[Pasted image 20220212115417.png]] - ![[Pasted image 20220212115523.png]] - ![[Pasted image 20220212115544.png]] - ![[Pasted image 20220212115628.png]] - This is because the $\\beta$ is the only unknown here - The question is **What is the best way to \"guess\"** the correct $\\beta$ values that minimize the gradient To Walkthrough an Single Feature Example We can use gradient descent to computationally search for the coefficeint that minimize this gradient. Let's visually explore what this looks like int he case of a single Beta value Given a cost function of \\(J(\\beta)\\) how can we computationally search for the correct value of \\(\\beta\\) that minimizes the gradient of the cost functions What would be search process look like for a single \\(\\beta\\) value ![[Pasted image 20220212120229.png]] ![[Pasted image 20220212120329.png]] The orange curve is the derivative of the cost function Steeper gradient at the start give larger steps Smaller gradient at the end gives smaller steps To Walkthrough an Two Feature Example ![[Pasted image 20220212120647.png]] ![[Pasted image 20220212120659.png]] --- Linear Regression in Python Simple Linear Regression Limited to one X feature To create a best-fit line to map out a linear relationship betweent total advertising spedn adn resulting sales X = df['total_spend'] y = df['sales'] help(np.polyfit) # Solve by Ordinaly Least Square # Return the coefficient y = B1X + B0 np.polyfit(x, y, deg=1) # array([0.04868788, 4.24302822]) np.polyfit(x, y, deg=3) # can use to fit multi-polynomial # y = B3x**3 + B2*x**2 + B1x +B0 Overview on Scikit-Learn Phiosophy of Scikit-Learn Focus on applying models and performance metrics This is more pragmatic industry style, rather than academic approach of describing the model and its parameter from sklearn.model_family import ModelAlgo mymodel = ModelAlgo(param1, param2) mymodel.fit(X_train, y_train) predictions = mymodel.predict (X_test) from sklearn.metrics import error_metric performance = error_metric(y_test, predictions) Evaluating Regression Mean Absolute Error ![[Pasted image 20220212122918.png]] Issues: Won't punish large errors ![[Pasted image 20220212123004.png]] Mean Squared Error ![[Pasted image 20220212123132.png]] Root Mean Square Error ![[Pasted image 20220212123144.png]] Residuals Good to separately evaluate residuals \\((y - \\hat{y})\\) , besides calculating performance metrics ![[Pasted image 20220212123602.png]] This visualization between X and Y won't work when dealing with more than one X feature We can plot residual error against true y values ![[Pasted image 20220212123816.png]] ![[Pasted image 20220212123908.png]] sns.scatterplot(x=y_test, y=test_residuals) plt.axhline(y=0, color='r', ls='--') sns.displot(test_residuals, bins=25, kde=True) Example 1: ![[Pasted image 20220212124026.png]] Example 2:![[Pasted image 20220212124120.png]] Normal Plot using Scipy![[Pasted image 20220212124401.png]] Model Deployment & Coefficients Interpretation ![[Pasted image 20220212124943.png]] ![[Pasted image 20220212124959.png]] Polynomial Regression Motivations Non-linear feature relationships to label Higher order transforms the relationship becomes more lienar To capture more signals in the data Much easier to find a beta coefficient thant the original features![[Pasted image 20220212130213.png]] Interaction terms between features What if features are only significant when in sync with one another Example given: Newspaper and TVs advertisement The simplest way is to create a new feature that multiplies two existing features together to create an interaction term . We can keep the original features, and add on this interaction term Scikit-learn does this with preprocessing . PolynomialFeatures automatically creates both higher order feature polynomials and the interaction terms between the all feature combinations The features created include The bias (the value of 1.0) Values raised to a power for each degree (e.g. x^1, x^2, x^3, ..) Interactions between all pairs of features (e.g. x1 * x2, x1 * x3, ...) Example Converting Two Features A and B 1, A, B, A, AB, B Generalized terms of features \\(X_1\\) and \\(X_2\\) 1, \\(X_1\\) , \\(X_2\\) , \\(X^2_1\\) , \\(X_1 X_2\\) , \\(X^2_2\\) Example if row was X=2 and X,=3 1,2,3, 4, 6, 9 --- Bias Variance Trade-Off Overfitting Low bias, high variance Harder to detect Underfitting Too much bias, Little Variance Related Concept: Parcemonial --- Regularization Regularization seeks to solve a few common model issues by Minimizing model complexity Penalizing the loss function Reducing model overfitting (add more bias to reduce model variance) In general, it is a way to reduce model overfitting and variance Require soem additional bias Require a search for a optimal penalty hyperparameter Here, covers the 3 main types of Regularization L1 Regularization L2 Regularization Combining L1 and L2 L1 Regularization ![[Pasted image 20220212171702.png]] ![[Pasted image 20220212171730.png]] Highlighted is the penalty term \\(\\lamda\\) is the hyperparameter title: LASSO Least Absolute Shrinkage and Selection Operator ![[Pasted image 20220214001828.png]] ![[Pasted image 20220214001933.png]] Example ![[Pasted image 20220215170400.png]] ![[Pasted image 20220215170540.png]] This works similar to the np.linspace, where the esp is the n_alphas defines how many a to test and smaller eps means smaller steps (finer search grid) thus longer computations documentation Hands-On ![[Pasted image 20220215170348.png]] L2 Regularization ![[Pasted image 20220212171858.png]] \\(RSS = \\sum^n_{i=1} (y_i - \\hat{y})\\) RSS = Residual Sum of Squares \\(\\lambda\\) can be any value from 0 to positive infinity ![[Pasted image 20220212171910.png]] ![[Pasted image 20220213003015.png]] Example title: Equation of the Linear Regression Not Change ![[Pasted image 20220213222055.png]] ![[Pasted image 20220213230432.png]] ![[Pasted image 20220213230457.png]] ![[Pasted image 20220214000554.png]] title: Note And if you're introducing this shrinkage term, which is trying to minimize this idea of beta squared,that means even small increases in beta one. Again, the slope of your line is going to really start to show up a lot more in this squared term andthen your land, the parameter would just be a tuning factor of how much do you want to punish by beta squared. But regardless of lambda, as long it's not equal to zero, we're still going to punish those larger slope's. So, again, for a single feature, this would actually lower your slope value. **So this is really the cusp of what Rich Regression is trying to do, is try to make sure that you're not overly responsive to your training data.** That way, when you unknown data comes in, you have a little more bias and you're going to generalize better to new unseen data. ![[Pasted image 20220214001042.png]] ![[Pasted image 20220214001231.png]] Hands-On ![[Pasted image 20220214001309.png]] from sklearn.linear_model import RidgeCV ridge_cv_model = RidgeCV(alphas=(0.1, 1.0, 10.0)) ridge_cv_model.fit(X_train, y_train) # Find out the name of SCORER from sklearn.metrics import SCORERS SCORERS.keys() L1 + L2 (Elastic Net) ![[Pasted image 20220212172208.png]] ![[Pasted image 20220212172225.png]] ![[Pasted image 20220212172248.png]] Below is the re-written version of Lasso and Ridge ![[Pasted image 20220215171913.png]] Example ![[Pasted image 20220215172132.png]] ![[Pasted image 20220215172146.png]] ![[Pasted image 20220215172251.png]] ![[Pasted image 20220215172311.png]] Residual sum of squares actually look like, we can think of the minimization path for the residual sum of squares as just lines we're essentially solving for trying to find the coefficients that minimize that loss function. So obviously, you would kind of start solving for these with **gradient descent** and you could begin to start solving these. But remember, you're going to be subject to that penalty term being Lessner equal to S. ![[Pasted image 20220215172408.png]] ![[Pasted image 20220215172530.png]] ![[Pasted image 20220215172621.png]] ![[Pasted image 20220215172653.png]] In this case, the corner of the squares for two dimensions means one of the coefficients on that corner is going to be zero, just by definition of how this is centered. So, again, to make it clear from a geometry perspective for LASO, we're dealing with **hyper cubes**. And in the simplest case, we're going to have just two features here to make that square. And it's highly likely if you come into contact with something in the shape of a square, that you're going to hit one of those corners, meaning one of those coefficients is going to be zero. This is why LASO is likely to lead to coefficients going all the way to zero. ![[Pasted image 20220215172924.png]] ![[Pasted image 20220215173005.png]] ![[Pasted image 20220215173026.png]] Note the two distinct \\(\\lambda\\) ![[Pasted image 20220215173102.png]] ![[Pasted image 20220215173132.png]] ![[Pasted image 20220215173159.png]] Hands-on ![[Pasted image 20220215173317.png]] ![[Pasted image 20220215173337.png]] --- Feature Scaling Feature scaling inmproves the convergence of steepest descent algorithms, which do not possess the property of scale invariance. If features are on different scales, certain weights may update faster than others since the feature values x, play a role in the weight updates. What is ideal is they should optimize in about the same time Scaling the features so that their respective ranges are uniform is important |in comparing measurements that have different units. Allows us directly compare model coefficients to each other. Feature scaling caveats: Must always scale neW unseen data before feeding to model. Effects direct interpretability of feature coefficients Easier to compare coefficients to one another, harder to relate back to original unscaled feature. Feature scaling benefits: Can lead to great increases in performance. Absolutely necessary for some models. Virtually no \"real\" downside to scaling features. Two main ways to scale features: Standardization Rescales data to have a mean of 0 and standard deviation of 1. Normalization Rescales all data values to be between 0-1. ![[Pasted image 20220212234516.png]] ![[Pasted image 20220212234542.png]] title: Feature Scaling Process - Feature Scaling Process - Perform train test split - Fit to training feature data - Transform training feature data - Transform test feature data ![[Pasted image 20220212234802.png]] title: Scaling Feature Impact SGD - Can negatively impact stochastic gradient descent - [here](https://stats.stackexchange.com/questions/111467) --- Cross Validation ![[Pasted image 20220212235558.png]] ![[Pasted image 20220212235634.png]] ![[Pasted image 20220212235711.png]] Header 1 Header 2","title":"Linear Regression"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Linear%20Regression/#overview","text":"![[Pasted image 20220208231922.png]] ![[Pasted image 20220212124652.png]]","title":"Overview"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Linear%20Regression/#understanding-ols","text":"using Simple Linear Regression as example OLS -> y=mx+b There is only room for one possible feature x At later part will see there is a need like gradient descent to scale this to multiple features ![[Pasted image 20220208233744.png]] ![[Pasted image 20211013004110.png]] ![[Pasted image 20220208233918.png]] ![[Pasted image 20220208233956.png]] title: y hat (Not y) Thereis usuallky no set of Betas to create a perfect fit to y!","title":"Understanding OLS"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Linear%20Regression/#formula-for-ols-intuitive","text":"![[msedge_EMtik5iNnt.png]] title: Intuitive Explanation for OLS Slope of the regression line is the weighted average of $$ \\frac{yi-\\bar{y}}{x^i - \\bar{x}}$$","title":"Formula for OLS (Intuitive)"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Linear%20Regression/#formula-for-ols-equation","text":"![[Pasted image 20220208235124.png]]","title":"Formula for OLS (Equation)"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Linear%20Regression/#limitations-of-linear-regression","text":"![[msedge_avdWgYmrsy.png]] ![[Pasted image 20211013001800.png]]","title":"Limitations of Linear Regression"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Linear%20Regression/#understand-cost-function","text":"What we know so far: - Linear Relationships - y mx+b - OLS - Solve simple linear regression - Not scalable for multiple features - Translating real data to Matrix Notation - Generalized formula for Beta coefficients","title":"Understand Cost Function"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Linear%20Regression/#mean-squared-error-mse","text":"title: Average Squared Error for **m** rows $$\\frac{1}{m}\\sum^m_{j=1}(y^j - \\hat{y}^j)^2$$ title: Cost Function Define by Squared Error $$ J(\\beta) = \\frac{1}{2m}\\sum^m_{j=1}(y^j - \\hat{y}^j)^2$$ Function below where the y_hat is replace with the generalized function $$ J(\\beta) = \\frac{1}{2m}\\sum^m_{j=1}(y^j - \\sum^n_{i=0}\\beta_ix^j_i)^2$$ - The 2m is because taking the derivative by setting to 0 (canccel with power of 2) title: Additional 1/2 is for convenience for derivative $$J(\\beta) = \\frac{1}{2m}\\sum^m_{j=1}(y^j - \\hat{y}^j)^2$$ Then to substitute y_hat $$J(\\beta) = \\frac{1}{2m}\\sum^m_{j=1}(y^j - \\sum^n_{i=0}\\beta_ix^j_i)^2$$ ![[Pasted image 20220212113805.png]] - Unforunately, it's not sacalable to try to get an analytical solution to minize this cost function - Later part will learn to use gradient descent to minimize the cost functiion here","title":"Mean Squared Error (MSE)"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Linear%20Regression/#gradient-descent","text":"","title":"Gradient Descent"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Linear%20Regression/#intro-to-linear-regression","text":"Now we have the Cost Function to minimzize Taking the cost function derivative and then solving for zero to get the set of Beta Coefficients wioll be too difficult to solve directly through analytical solution Instead we describe this Cost Function through vectorized matrix notation and use gradient descent to have a computer figure out the set of Beta Coefficient values that mininize the cost/loss function . title: Recall the Cost Function (Rewritten) - where $k$ is the number of features $$ \\frac{\\partial J}{\\partial a\\beta_k}(\\beta) = \\frac{1}{m} \\sum^m_{j=1} (y^j - \\sum^n_{i=0}\\beta_i x^j_i) (-x^j_k)$$ - Also recall the data is in the form of a **matrix X** with a **vector of labels y** - Whcih means if we need a $\\beta$ for each feature, we can express a vector $\\beta$ value - That means we can plugin our equation of the derivative of the loss function - ![[Pasted image 20220212115241.png]] - ![[Pasted image 20220212115417.png]] - ![[Pasted image 20220212115523.png]] - ![[Pasted image 20220212115544.png]] - ![[Pasted image 20220212115628.png]] - This is because the $\\beta$ is the only unknown here - The question is **What is the best way to \"guess\"** the correct $\\beta$ values that minimize the gradient","title":"Intro to Linear Regression"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Linear%20Regression/#to-walkthrough-an-single-feature-example","text":"We can use gradient descent to computationally search for the coefficeint that minimize this gradient. Let's visually explore what this looks like int he case of a single Beta value Given a cost function of \\(J(\\beta)\\) how can we computationally search for the correct value of \\(\\beta\\) that minimizes the gradient of the cost functions What would be search process look like for a single \\(\\beta\\) value ![[Pasted image 20220212120229.png]] ![[Pasted image 20220212120329.png]] The orange curve is the derivative of the cost function Steeper gradient at the start give larger steps Smaller gradient at the end gives smaller steps","title":"To Walkthrough an Single Feature Example"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Linear%20Regression/#to-walkthrough-an-two-feature-example","text":"![[Pasted image 20220212120647.png]] ![[Pasted image 20220212120659.png]]","title":"To Walkthrough an Two Feature Example"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Linear%20Regression/#-","text":"","title":"---"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Linear%20Regression/#linear-regression-in-python","text":"","title":"Linear Regression in Python"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Linear%20Regression/#simple-linear-regression","text":"Limited to one X feature To create a best-fit line to map out a linear relationship betweent total advertising spedn adn resulting sales X = df['total_spend'] y = df['sales'] help(np.polyfit) # Solve by Ordinaly Least Square # Return the coefficient y = B1X + B0 np.polyfit(x, y, deg=1) # array([0.04868788, 4.24302822]) np.polyfit(x, y, deg=3) # can use to fit multi-polynomial # y = B3x**3 + B2*x**2 + B1x +B0","title":"Simple Linear Regression"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Linear%20Regression/#overview-on-scikit-learn","text":"Phiosophy of Scikit-Learn Focus on applying models and performance metrics This is more pragmatic industry style, rather than academic approach of describing the model and its parameter from sklearn.model_family import ModelAlgo mymodel = ModelAlgo(param1, param2) mymodel.fit(X_train, y_train) predictions = mymodel.predict (X_test) from sklearn.metrics import error_metric performance = error_metric(y_test, predictions)","title":"Overview on Scikit-Learn"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Linear%20Regression/#evaluating-regression","text":"","title":"Evaluating Regression"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Linear%20Regression/#mean-absolute-error","text":"![[Pasted image 20220212122918.png]] Issues: Won't punish large errors ![[Pasted image 20220212123004.png]]","title":"Mean Absolute Error"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Linear%20Regression/#mean-squared-error","text":"![[Pasted image 20220212123132.png]]","title":"Mean Squared Error"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Linear%20Regression/#root-mean-square-error","text":"![[Pasted image 20220212123144.png]]","title":"Root Mean Square Error"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Linear%20Regression/#residuals","text":"Good to separately evaluate residuals \\((y - \\hat{y})\\) , besides calculating performance metrics ![[Pasted image 20220212123602.png]] This visualization between X and Y won't work when dealing with more than one X feature We can plot residual error against true y values ![[Pasted image 20220212123816.png]] ![[Pasted image 20220212123908.png]] sns.scatterplot(x=y_test, y=test_residuals) plt.axhline(y=0, color='r', ls='--') sns.displot(test_residuals, bins=25, kde=True) Example 1: ![[Pasted image 20220212124026.png]] Example 2:![[Pasted image 20220212124120.png]] Normal Plot using Scipy![[Pasted image 20220212124401.png]]","title":"Residuals"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Linear%20Regression/#model-deployment-coefficients-interpretation","text":"![[Pasted image 20220212124943.png]] ![[Pasted image 20220212124959.png]]","title":"Model Deployment &amp; Coefficients Interpretation"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Linear%20Regression/#polynomial-regression","text":"Motivations Non-linear feature relationships to label Higher order transforms the relationship becomes more lienar To capture more signals in the data Much easier to find a beta coefficient thant the original features![[Pasted image 20220212130213.png]] Interaction terms between features What if features are only significant when in sync with one another Example given: Newspaper and TVs advertisement The simplest way is to create a new feature that multiplies two existing features together to create an interaction term . We can keep the original features, and add on this interaction term Scikit-learn does this with preprocessing . PolynomialFeatures automatically creates both higher order feature polynomials and the interaction terms between the all feature combinations The features created include The bias (the value of 1.0) Values raised to a power for each degree (e.g. x^1, x^2, x^3, ..) Interactions between all pairs of features (e.g. x1 * x2, x1 * x3, ...) Example Converting Two Features A and B 1, A, B, A, AB, B Generalized terms of features \\(X_1\\) and \\(X_2\\) 1, \\(X_1\\) , \\(X_2\\) , \\(X^2_1\\) , \\(X_1 X_2\\) , \\(X^2_2\\) Example if row was X=2 and X,=3 1,2,3, 4, 6, 9","title":"Polynomial Regression"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Linear%20Regression/#-_1","text":"","title":"---"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Linear%20Regression/#bias-variance-trade-off","text":"Overfitting Low bias, high variance Harder to detect Underfitting Too much bias, Little Variance Related Concept: Parcemonial","title":"Bias Variance Trade-Off"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Linear%20Regression/#-_2","text":"","title":"---"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Linear%20Regression/#regularization","text":"Regularization seeks to solve a few common model issues by Minimizing model complexity Penalizing the loss function Reducing model overfitting (add more bias to reduce model variance) In general, it is a way to reduce model overfitting and variance Require soem additional bias Require a search for a optimal penalty hyperparameter Here, covers the 3 main types of Regularization L1 Regularization L2 Regularization Combining L1 and L2","title":"Regularization"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Linear%20Regression/#l1-regularization","text":"![[Pasted image 20220212171702.png]] ![[Pasted image 20220212171730.png]] Highlighted is the penalty term \\(\\lamda\\) is the hyperparameter title: LASSO Least Absolute Shrinkage and Selection Operator ![[Pasted image 20220214001828.png]] ![[Pasted image 20220214001933.png]]","title":"L1 Regularization"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Linear%20Regression/#example","text":"![[Pasted image 20220215170400.png]] ![[Pasted image 20220215170540.png]] This works similar to the np.linspace, where the esp is the n_alphas defines how many a to test and smaller eps means smaller steps (finer search grid) thus longer computations documentation","title":"Example"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Linear%20Regression/#hands-on","text":"![[Pasted image 20220215170348.png]]","title":"Hands-On"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Linear%20Regression/#l2-regularization","text":"![[Pasted image 20220212171858.png]] \\(RSS = \\sum^n_{i=1} (y_i - \\hat{y})\\) RSS = Residual Sum of Squares \\(\\lambda\\) can be any value from 0 to positive infinity ![[Pasted image 20220212171910.png]] ![[Pasted image 20220213003015.png]]","title":"L2 Regularization"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Linear%20Regression/#example_1","text":"title: Equation of the Linear Regression Not Change ![[Pasted image 20220213222055.png]] ![[Pasted image 20220213230432.png]] ![[Pasted image 20220213230457.png]] ![[Pasted image 20220214000554.png]] title: Note And if you're introducing this shrinkage term, which is trying to minimize this idea of beta squared,that means even small increases in beta one. Again, the slope of your line is going to really start to show up a lot more in this squared term andthen your land, the parameter would just be a tuning factor of how much do you want to punish by beta squared. But regardless of lambda, as long it's not equal to zero, we're still going to punish those larger slope's. So, again, for a single feature, this would actually lower your slope value. **So this is really the cusp of what Rich Regression is trying to do, is try to make sure that you're not overly responsive to your training data.** That way, when you unknown data comes in, you have a little more bias and you're going to generalize better to new unseen data. ![[Pasted image 20220214001042.png]] ![[Pasted image 20220214001231.png]]","title":"Example"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Linear%20Regression/#hands-on_1","text":"![[Pasted image 20220214001309.png]] from sklearn.linear_model import RidgeCV ridge_cv_model = RidgeCV(alphas=(0.1, 1.0, 10.0)) ridge_cv_model.fit(X_train, y_train) # Find out the name of SCORER from sklearn.metrics import SCORERS SCORERS.keys()","title":"Hands-On"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Linear%20Regression/#l1-l2-elastic-net","text":"![[Pasted image 20220212172208.png]] ![[Pasted image 20220212172225.png]] ![[Pasted image 20220212172248.png]] Below is the re-written version of Lasso and Ridge ![[Pasted image 20220215171913.png]]","title":"L1 + L2 (Elastic Net)"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Linear%20Regression/#example_2","text":"![[Pasted image 20220215172132.png]] ![[Pasted image 20220215172146.png]] ![[Pasted image 20220215172251.png]] ![[Pasted image 20220215172311.png]] Residual sum of squares actually look like, we can think of the minimization path for the residual sum of squares as just lines we're essentially solving for trying to find the coefficients that minimize that loss function. So obviously, you would kind of start solving for these with **gradient descent** and you could begin to start solving these. But remember, you're going to be subject to that penalty term being Lessner equal to S. ![[Pasted image 20220215172408.png]] ![[Pasted image 20220215172530.png]] ![[Pasted image 20220215172621.png]] ![[Pasted image 20220215172653.png]] In this case, the corner of the squares for two dimensions means one of the coefficients on that corner is going to be zero, just by definition of how this is centered. So, again, to make it clear from a geometry perspective for LASO, we're dealing with **hyper cubes**. And in the simplest case, we're going to have just two features here to make that square. And it's highly likely if you come into contact with something in the shape of a square, that you're going to hit one of those corners, meaning one of those coefficients is going to be zero. This is why LASO is likely to lead to coefficients going all the way to zero. ![[Pasted image 20220215172924.png]] ![[Pasted image 20220215173005.png]] ![[Pasted image 20220215173026.png]] Note the two distinct \\(\\lambda\\) ![[Pasted image 20220215173102.png]] ![[Pasted image 20220215173132.png]] ![[Pasted image 20220215173159.png]]","title":"Example"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Linear%20Regression/#hands-on_2","text":"![[Pasted image 20220215173317.png]] ![[Pasted image 20220215173337.png]]","title":"Hands-on"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Linear%20Regression/#-_3","text":"","title":"---"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Linear%20Regression/#feature-scaling","text":"Feature scaling inmproves the convergence of steepest descent algorithms, which do not possess the property of scale invariance. If features are on different scales, certain weights may update faster than others since the feature values x, play a role in the weight updates. What is ideal is they should optimize in about the same time Scaling the features so that their respective ranges are uniform is important |in comparing measurements that have different units. Allows us directly compare model coefficients to each other. Feature scaling caveats: Must always scale neW unseen data before feeding to model. Effects direct interpretability of feature coefficients Easier to compare coefficients to one another, harder to relate back to original unscaled feature. Feature scaling benefits: Can lead to great increases in performance. Absolutely necessary for some models. Virtually no \"real\" downside to scaling features. Two main ways to scale features: Standardization Rescales data to have a mean of 0 and standard deviation of 1. Normalization Rescales all data values to be between 0-1. ![[Pasted image 20220212234516.png]] ![[Pasted image 20220212234542.png]] title: Feature Scaling Process - Feature Scaling Process - Perform train test split - Fit to training feature data - Transform training feature data - Transform test feature data ![[Pasted image 20220212234802.png]] title: Scaling Feature Impact SGD - Can negatively impact stochastic gradient descent - [here](https://stats.stackexchange.com/questions/111467)","title":"Feature Scaling"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Linear%20Regression/#-_4","text":"","title":"---"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Linear%20Regression/#cross-validation","text":"![[Pasted image 20220212235558.png]] ![[Pasted image 20220212235634.png]] ![[Pasted image 20220212235711.png]] Header 1 Header 2","title":"Cross Validation"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Logistic%20Regression/","text":"Overview Transforming Linear Regression to Logistic Regression Mathematical Theory behidn Logistic Regression Simple implementation of Logistic Regression for Classification Problem Interpreting Results Odds Ratio and Coefficients Classification Metrics ROC Curves Multiclass classification with logistic regression --- [[#Overview|Overview]] 1. [[#1.0 Theory:|1.0 Theory:]][[#1.1 Logistic Function|1.1 Logistic Function]] 1. [[#1.2 Linear to Logistic Math|1.2 Linear to Logistic Math]] 1. [[#1.2.1 Odds|1.2.1 Odds]] 2. [[#1.2.2 Maximum Likelihood on How to Fit the Curve|1.2.2 Maximum Likelihood on How to Fit the Curve]] 2. [[#2.0 Classification Metrics|2.0 Classification Metrics]] 1. [[#2.1 Confusion Matrix & Accuracy|2.1 Confusion Matrix & Accuracy]] 2. [[#2.2 Precision, Recall, and F1-Score|2.2 Precision, Recall, and F1-Score]] 3. [[#2.3 ROC Curves|2.3 ROC Curves]] 1.0 Theory: 1.1 Logistic Function $$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$ ![[Pasted image 20220216224657.png]] 1.2 Linear to Logistic Math ![[Pasted image 20220216225044.png]] ![[Pasted image 20220216225058.png]] To be able to interpret the beta coefficient in Logistic Regression, there two terms we need to know: 1.2.1 Odds Often referred to as N to 1 The odds of an event with probability \\(p\\) is defined as the chance of the event happening divided by the chance of the event not happening : \\(\\frac{p}{1-p}\\) This allow us to solve for coefficients and feature x in terms of log odds To solve for log odds, divided by denominator ![[Pasted image 20220216225935.png]] Well, there's essentially two steps being shown here. I'm going to divide both sides by Y hat and then I can simply do the reciprocal or flip the sign here, because, again, you'll notice that on the left hand side, I start with either the power of negative and then the some of the beta coefficient times, the feature set, which means if I reverse that signs that it's E to the power of positive, the sum of the coefficients times X, I just need to take the reciprocal or flip that fraction over to get that relationship. ![[Pasted image 20220216225817.png]] ![[Pasted image 20220216230430.png]] we know that we can get rid of an E by canceling it with the natural log, which is the log odds, and keep in mind, sometimes you'll see this written out as just log. So that essentially implies that the base of this logarithm is natural ![[Pasted image 20220216230555.png]] ![[Pasted image 20220216230943.png]] So what I have inside that natural log are my odds and I'm just taking the log of those odds. And that's equal to this linear combination of the coefficients times, the feature on the right hand ![[Pasted image 20220216231235.png]] So now, if we consider is equal to zero point five or halfway point in terms of log odds is now equal to zero. Then you'll realize that as you keep getting closer and closer to one, the natural log of these odds is actually going to go to infinity. So the probability gets closer to one. You essentially have one divided by a very, very small number. And the natural log of that starts to go towards infinity. Likewise, as your probability goes to zero, then you're essentially saying zero divided by something very close to one which is starting to go to negative infinity. So in terms of log odds, we actually now have the points at infinity and negative infinity because these points that already define they have values of zero and one. Keep in mind, these are the actual data points that we're talking about here, not our predictions. So if we actually then convert that logistic curve to be on a log odd scale, it looks like a straight line. And the coefficients in terms are now in the change in log odds, so essentially all I'm saying here s that if I show this equation in terms of a linear relationship between beta coefficient and the X feature, then y y axis has to then be in terms of log odds. And because we're converting things to log odds, my y axis goes from negative infinity to positive infinity, where the class points that are already defined are at negative infinity and positive infinity. So this is essentially transforming that squiggle, if you will, or that curve of the logistic function into a straight line. But that did come at the cost of having to transform the Y axis into log odds that go from negativ infinity to positive infinity. So now that I did this, the question arises, was all this work worth it? Is the beta coefficient actually simple to interpret now? ![[Pasted image 20220216231842.png]] ![[Pasted image 20220216231927.png]] ![[Pasted image 20220216232015.png]] ![[Pasted image 20220216232039.png]] ![[Pasted image 20220216232107.png]] 1.2.2 Maximum Likelihood on How to Fit the Curve Details of maximum likelihood are out the scope ![[Pasted image 20220216232439.png]] ![[Pasted image 20220216232638.png]] ![[Pasted image 20220216232658.png]] ![[Pasted image 20220216232721.png]] This equation is the chart on the right $$ \\ln(\\frac{p}{1-p}) = \\ln(odds)$$ ![[Pasted image 20220216233402.png]] ![[Pasted image 20220216233437.png]] The last question is the chart of the left \\(\\hat{y}\\) ![[Pasted image 20220216233817.png]] - So I just choose some line to start off with along the log odds, axes and instead of residual sum of squares, because I can't really do that with negative infinity and positive infinity, what I'm going to do is take the X values and project them onto this line. - And recall, this is just == a line representing the beta coefficient times that feature value of X ==. And when I project these points onto this line, they're no longer all the way at negative infinity and infinity, which means I can actually calculate the log odds for the projected points on this line. - And you should recall that once I have things in terms of log odds, I can convert these directly to probabilities onto my logistic regression model. Perfect fit: all blue at the top at 1; all red at the bottom at 0 Likelihood = Product of Probabilities of Belonging to Class 1 ![[Pasted image 20220216234139.png]] The perfect likelihood should be 1 The calculated likelihood is the value to be maximized ![[Pasted image 20220216234231.png]] ![[Pasted image 20220216234258.png]] - As a quick side note, in practice, we actually maximized the log of the likelihoods, that is to say there would be a log or natural log on each of those probabilities, zero point nine zero point, etc. - So it'd be the exact same math we saw before, except everything would go inside a log. And that's just because the mathematics ends up working a lot nicer when it comes to things like the minimization function and the gradient descent. - But the idea and intuition is the same. Maximizing the likelihoods is the same as maximizing the log of the likelihoods. ![[Pasted image 20220216234424.png]] - Now, if we think back to this projection along the log odds y axis, I know that there's going to be some set of beta coefficients that is going to maximize these log likelihoods. And that's essentially all we're doing. - I'm choosing the best coefficient values in log odds terms.That creates the maximum likelihood as far as predicting the correct class calls for these trading data points. - And so what I can do is start experimenting with these different lines, essentially continually adjusting this line if it calculate those maximum likelihoods and continue this until I find the best fitting set of coefficient values. - ![[Pasted image 20220216235906.png]] - So a quick note again on the mathematics, while we are actually trying to maximize the likelihood from a computer science standpoint and as far as the algorithms are concerned, we still need something to minimize because greed, the methods can really only search for Minimum's. - It's extremely difficult to search for a maximum because you essentially have a gradient that explodesand it's almost impossible to figure out if you've actually reached a max value. - However, because of derivatives, it's much easier for minimum's with stochastic gradient descent. So that means if we actually go through the math and formulate this all out in terms of a cost function, we seek to minimize the following. And formally this is known as log loss. - ![[Pasted image 20220216235954.png]] --- 2.0 Classification Metrics 2.1 Confusion Matrix & Accuracy ![[Pasted image 20220217000243.png]] ![[Pasted image 20220217000332.png]] ![[Pasted image 20220217000351.png]] 2.2 Precision, Recall, and F1-Score ![[Pasted image 20220217000527.png]] ![[Pasted image 20220217000541.png]] 2.3 ROC Curves ![[Pasted image 20220217000725.png]] ![[Pasted image 20220217000854.png]] ![[Pasted image 20220217000923.png]] ![[Pasted image 20220217000936.png]] ![[Pasted image 20220217001137.png]] ![[Pasted image 20220217001217.png]] ![[Pasted image 20220217001405.png]] Plot Multiclass ROC","title":"Logistic Regression"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Logistic%20Regression/#overview","text":"Transforming Linear Regression to Logistic Regression Mathematical Theory behidn Logistic Regression Simple implementation of Logistic Regression for Classification Problem Interpreting Results Odds Ratio and Coefficients Classification Metrics ROC Curves Multiclass classification with logistic regression","title":"Overview"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Logistic%20Regression/#-","text":"[[#Overview|Overview]] 1. [[#1.0 Theory:|1.0 Theory:]][[#1.1 Logistic Function|1.1 Logistic Function]] 1. [[#1.2 Linear to Logistic Math|1.2 Linear to Logistic Math]] 1. [[#1.2.1 Odds|1.2.1 Odds]] 2. [[#1.2.2 Maximum Likelihood on How to Fit the Curve|1.2.2 Maximum Likelihood on How to Fit the Curve]] 2. [[#2.0 Classification Metrics|2.0 Classification Metrics]] 1. [[#2.1 Confusion Matrix & Accuracy|2.1 Confusion Matrix & Accuracy]] 2. [[#2.2 Precision, Recall, and F1-Score|2.2 Precision, Recall, and F1-Score]] 3. [[#2.3 ROC Curves|2.3 ROC Curves]]","title":"---"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Logistic%20Regression/#10-theory","text":"","title":"1.0 Theory:"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Logistic%20Regression/#11-logistic-function","text":"$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$ ![[Pasted image 20220216224657.png]]","title":"1.1 Logistic Function"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Logistic%20Regression/#12-linear-to-logistic-math","text":"![[Pasted image 20220216225044.png]] ![[Pasted image 20220216225058.png]] To be able to interpret the beta coefficient in Logistic Regression, there two terms we need to know:","title":"1.2 Linear to Logistic Math"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Logistic%20Regression/#121-odds","text":"Often referred to as N to 1 The odds of an event with probability \\(p\\) is defined as the chance of the event happening divided by the chance of the event not happening : \\(\\frac{p}{1-p}\\) This allow us to solve for coefficients and feature x in terms of log odds To solve for log odds, divided by denominator ![[Pasted image 20220216225935.png]] Well, there's essentially two steps being shown here. I'm going to divide both sides by Y hat and then I can simply do the reciprocal or flip the sign here, because, again, you'll notice that on the left hand side, I start with either the power of negative and then the some of the beta coefficient times, the feature set, which means if I reverse that signs that it's E to the power of positive, the sum of the coefficients times X, I just need to take the reciprocal or flip that fraction over to get that relationship. ![[Pasted image 20220216225817.png]] ![[Pasted image 20220216230430.png]] we know that we can get rid of an E by canceling it with the natural log, which is the log odds, and keep in mind, sometimes you'll see this written out as just log. So that essentially implies that the base of this logarithm is natural ![[Pasted image 20220216230555.png]] ![[Pasted image 20220216230943.png]] So what I have inside that natural log are my odds and I'm just taking the log of those odds. And that's equal to this linear combination of the coefficients times, the feature on the right hand ![[Pasted image 20220216231235.png]] So now, if we consider is equal to zero point five or halfway point in terms of log odds is now equal to zero. Then you'll realize that as you keep getting closer and closer to one, the natural log of these odds is actually going to go to infinity. So the probability gets closer to one. You essentially have one divided by a very, very small number. And the natural log of that starts to go towards infinity. Likewise, as your probability goes to zero, then you're essentially saying zero divided by something very close to one which is starting to go to negative infinity. So in terms of log odds, we actually now have the points at infinity and negative infinity because these points that already define they have values of zero and one. Keep in mind, these are the actual data points that we're talking about here, not our predictions. So if we actually then convert that logistic curve to be on a log odd scale, it looks like a straight line. And the coefficients in terms are now in the change in log odds, so essentially all I'm saying here s that if I show this equation in terms of a linear relationship between beta coefficient and the X feature, then y y axis has to then be in terms of log odds. And because we're converting things to log odds, my y axis goes from negative infinity to positive infinity, where the class points that are already defined are at negative infinity and positive infinity. So this is essentially transforming that squiggle, if you will, or that curve of the logistic function into a straight line. But that did come at the cost of having to transform the Y axis into log odds that go from negativ infinity to positive infinity. So now that I did this, the question arises, was all this work worth it? Is the beta coefficient actually simple to interpret now? ![[Pasted image 20220216231842.png]] ![[Pasted image 20220216231927.png]] ![[Pasted image 20220216232015.png]] ![[Pasted image 20220216232039.png]] ![[Pasted image 20220216232107.png]]","title":"1.2.1 Odds"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Logistic%20Regression/#122-maximum-likelihood-on-how-to-fit-the-curve","text":"Details of maximum likelihood are out the scope ![[Pasted image 20220216232439.png]] ![[Pasted image 20220216232638.png]] ![[Pasted image 20220216232658.png]] ![[Pasted image 20220216232721.png]] This equation is the chart on the right $$ \\ln(\\frac{p}{1-p}) = \\ln(odds)$$ ![[Pasted image 20220216233402.png]] ![[Pasted image 20220216233437.png]] The last question is the chart of the left \\(\\hat{y}\\) ![[Pasted image 20220216233817.png]] - So I just choose some line to start off with along the log odds, axes and instead of residual sum of squares, because I can't really do that with negative infinity and positive infinity, what I'm going to do is take the X values and project them onto this line. - And recall, this is just == a line representing the beta coefficient times that feature value of X ==. And when I project these points onto this line, they're no longer all the way at negative infinity and infinity, which means I can actually calculate the log odds for the projected points on this line. - And you should recall that once I have things in terms of log odds, I can convert these directly to probabilities onto my logistic regression model. Perfect fit: all blue at the top at 1; all red at the bottom at 0 Likelihood = Product of Probabilities of Belonging to Class 1 ![[Pasted image 20220216234139.png]] The perfect likelihood should be 1 The calculated likelihood is the value to be maximized ![[Pasted image 20220216234231.png]] ![[Pasted image 20220216234258.png]] - As a quick side note, in practice, we actually maximized the log of the likelihoods, that is to say there would be a log or natural log on each of those probabilities, zero point nine zero point, etc. - So it'd be the exact same math we saw before, except everything would go inside a log. And that's just because the mathematics ends up working a lot nicer when it comes to things like the minimization function and the gradient descent. - But the idea and intuition is the same. Maximizing the likelihoods is the same as maximizing the log of the likelihoods. ![[Pasted image 20220216234424.png]] - Now, if we think back to this projection along the log odds y axis, I know that there's going to be some set of beta coefficients that is going to maximize these log likelihoods. And that's essentially all we're doing. - I'm choosing the best coefficient values in log odds terms.That creates the maximum likelihood as far as predicting the correct class calls for these trading data points. - And so what I can do is start experimenting with these different lines, essentially continually adjusting this line if it calculate those maximum likelihoods and continue this until I find the best fitting set of coefficient values. - ![[Pasted image 20220216235906.png]] - So a quick note again on the mathematics, while we are actually trying to maximize the likelihood from a computer science standpoint and as far as the algorithms are concerned, we still need something to minimize because greed, the methods can really only search for Minimum's. - It's extremely difficult to search for a maximum because you essentially have a gradient that explodesand it's almost impossible to figure out if you've actually reached a max value. - However, because of derivatives, it's much easier for minimum's with stochastic gradient descent. So that means if we actually go through the math and formulate this all out in terms of a cost function, we seek to minimize the following. And formally this is known as log loss. - ![[Pasted image 20220216235954.png]]","title":"1.2.2 Maximum Likelihood on How to Fit the Curve"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Logistic%20Regression/#-_1","text":"","title":"---"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Logistic%20Regression/#20-classification-metrics","text":"","title":"2.0 Classification Metrics"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Logistic%20Regression/#21-confusion-matrix-accuracy","text":"![[Pasted image 20220217000243.png]] ![[Pasted image 20220217000332.png]] ![[Pasted image 20220217000351.png]]","title":"2.1 Confusion Matrix &amp; Accuracy"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Logistic%20Regression/#22-precision-recall-and-f1-score","text":"![[Pasted image 20220217000527.png]] ![[Pasted image 20220217000541.png]]","title":"2.2 Precision, Recall, and F1-Score"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Logistic%20Regression/#23-roc-curves","text":"![[Pasted image 20220217000725.png]] ![[Pasted image 20220217000854.png]] ![[Pasted image 20220217000923.png]] ![[Pasted image 20220217000936.png]] ![[Pasted image 20220217001137.png]] ![[Pasted image 20220217001217.png]] ![[Pasted image 20220217001405.png]] Plot Multiclass ROC","title":"2.3 ROC Curves"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Python%20for%20Machine%20Learning%20Masterclass/","text":"Python for Machine Learning Masterclass Overview type: folder_brief_live","title":"Python for Machine Learning Masterclass Overview"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Python%20for%20Machine%20Learning%20Masterclass/#python-for-machine-learning-masterclass-overview","text":"type: folder_brief_live","title":"Python for Machine Learning Masterclass Overview"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Support%20Vector%20Machines/","text":"[[#1. Overview|1. Overview]] [[#2.Theory & Intuition|2.Theory & Intuition]] [[#2.1 Hyperplanes & Margins|2.1 Hyperplanes & Margins]] [[#2.2 Maximal Margin Classifier|2.2 Maximal Margin Classifier]] [[#2.3 Support Vector Classifier (Soft Margin)|2.3 Support Vector Classifier (Soft Margin)]] [[#2.4 Support Vector Machine|2.4 Support Vector Machine]] --- 1. Overview Support Vector Machines are one of the more complex algorithms we will learn, but it all begins with a simple premise: Does a hyperplane exist that can effectively separate classes? To answer this question, we first need to go through the history and development of Support Vector Machines! Section Overview History Intuition and Theory for SVM SVM Classification Example SVM Regression Example SVM Project Exercise and Solutions --- 2.Theory & Intuition 2.1 Hyperplanes & Margins Steps to build up to SVMs Maximum Margin Classifier Support Vector Classifier Support Vector Machine ![[Pasted image 20220218001324.png]] 2.2 Maximal Margin Classifier ![[Pasted image 20220218203545.png]] ![[Pasted image 20220218203616.png]] ![[Pasted image 20220218203631.png]] ![[Pasted image 20220218203646.png]] This same idea of maximum margin applies to N dimensions Below is the example of 2-dimensions ![[Pasted image 20220218204304.png]] ![[Pasted image 20220218204502.png]] 2.3 Support Vector Classifier (Soft Margin) ![[Pasted image 20220218204542.png]] ![[Pasted image 20220218204735.png]] ![[Pasted image 20220218204830.png]] ![[Pasted image 20220218204914.png]] No misclassification ![[Pasted image 20220218204933.png]] With misclassification allowed. This introduces higher bias, and lower variance (to prevent overfitting) Examples so far are based on easily separable classes. Next we use example where hyperplane won't separate the classes without any missclassification ![[Pasted image 20220218205232.png]] ![[Pasted image 20220218205243.png]] 2.4 Support Vector Machine ![[Pasted image 20220219175937.png]] ![[Pasted image 20220219180836.png]] ![[Pasted image 20220219180854.png]] ![[Pasted image 20220219180930.png]] ![[Pasted image 20220219180948.png]] 2-dimension example starts from here ![[Pasted image 20220219181024.png]] ![[Pasted image 20220219181345.png]] You may have heard of the use of kernels in SVM as the ==\"kernel trick\"==. We previously visualized transforming data points from one dimension into a higheer dimension. Mathematically, the kernel trick actually avoids recomputing the points in a higher dimensional space! How does the kernel trick achieve this? It takes advantage of dot products of the transpositions of the data. In the next lecture, we will go through the basic mathematical ideas behind the \"kernel trick\"! 2.5 Kernel Trick & Mathematics ![[Pasted image 20220219184736.png]] ![[Pasted image 20220219184810.png]] ![[Pasted image 20220219184818.png]]","title":"Support Vector Machines"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Support%20Vector%20Machines/#-","text":"","title":"---"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Support%20Vector%20Machines/#1-overview","text":"Support Vector Machines are one of the more complex algorithms we will learn, but it all begins with a simple premise: Does a hyperplane exist that can effectively separate classes? To answer this question, we first need to go through the history and development of Support Vector Machines! Section Overview History Intuition and Theory for SVM SVM Classification Example SVM Regression Example SVM Project Exercise and Solutions","title":"1. Overview"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Support%20Vector%20Machines/#-_1","text":"","title":"---"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Support%20Vector%20Machines/#2theory-intuition","text":"","title":"2.Theory &amp; Intuition"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Support%20Vector%20Machines/#21-hyperplanes-margins","text":"Steps to build up to SVMs Maximum Margin Classifier Support Vector Classifier Support Vector Machine ![[Pasted image 20220218001324.png]]","title":"2.1 Hyperplanes &amp; Margins"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Support%20Vector%20Machines/#22-maximal-margin-classifier","text":"![[Pasted image 20220218203545.png]] ![[Pasted image 20220218203616.png]] ![[Pasted image 20220218203631.png]] ![[Pasted image 20220218203646.png]] This same idea of maximum margin applies to N dimensions Below is the example of 2-dimensions ![[Pasted image 20220218204304.png]] ![[Pasted image 20220218204502.png]]","title":"2.2 Maximal Margin Classifier"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Support%20Vector%20Machines/#23-support-vector-classifier-soft-margin","text":"![[Pasted image 20220218204542.png]] ![[Pasted image 20220218204735.png]] ![[Pasted image 20220218204830.png]] ![[Pasted image 20220218204914.png]] No misclassification ![[Pasted image 20220218204933.png]] With misclassification allowed. This introduces higher bias, and lower variance (to prevent overfitting) Examples so far are based on easily separable classes. Next we use example where hyperplane won't separate the classes without any missclassification ![[Pasted image 20220218205232.png]] ![[Pasted image 20220218205243.png]]","title":"2.3 Support Vector Classifier (Soft Margin)"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Support%20Vector%20Machines/#24-support-vector-machine","text":"![[Pasted image 20220219175937.png]] ![[Pasted image 20220219180836.png]] ![[Pasted image 20220219180854.png]] ![[Pasted image 20220219180930.png]] ![[Pasted image 20220219180948.png]] 2-dimension example starts from here ![[Pasted image 20220219181024.png]] ![[Pasted image 20220219181345.png]] You may have heard of the use of kernels in SVM as the ==\"kernel trick\"==. We previously visualized transforming data points from one dimension into a higheer dimension. Mathematically, the kernel trick actually avoids recomputing the points in a higher dimensional space! How does the kernel trick achieve this? It takes advantage of dot products of the transpositions of the data. In the next lecture, we will go through the basic mathematical ideas behind the \"kernel trick\"!","title":"2.4 Support Vector Machine"},{"location":"Python%20for%20Machine%20Learning%20Masterclass/Support%20Vector%20Machines/#25-kernel-trick-mathematics","text":"![[Pasted image 20220219184736.png]] ![[Pasted image 20220219184810.png]] ![[Pasted image 20220219184818.png]]","title":"2.5 Kernel Trick &amp; Mathematics"}]}